{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Footprint Segmentation from Satellite images (bfss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is an attempt to walk through the entire code step-by-step, explaining the different blocks, to give an overview of the porject. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/pradip.gupta/personal-projects/bfss/notebook'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/pradip.gupta/personal-projects/bfss'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, glob, shutil\n",
    "os.chdir(os.path.dirname(os.getcwd()))\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding \"src/networks\" folder in path, to enable in-line imports for the network files using importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath('./src/networks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#To handel OOM errors\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as ktf\n",
    "def get_session():\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction= 0.9,\n",
    "                                allow_growth=True)\n",
    "    return tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "ktf.set_session(get_session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard imports\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.optimizers import Adam, RMSprop, Nadam, SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading all the custom functions that we have written. These make the training script neat and help in debugging in case of errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom imports\n",
    "import config\n",
    "from src.training import data_loader\n",
    "from src.training.metrics import bce_dice_loss, dice_coeff\n",
    "from src.training.seg_data_generator import SegDataGenerator\n",
    "from src.training.keras_callbacks import get_callbacks\n",
    "from src.training.modeller import finetune_model\n",
    "from src.training.keras_history import generate_stats\n",
    "from src.training.plots import save_plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data from the dataset path. Here we are loading only the meta data for all the AOIs. The X and Y variables hold a buzzard datasource object. \n",
    "\n",
    "Get to know more on buzzard here: https://github.com/airware/buzzard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of aois:\n",
      "for train: 14\n",
      "for validation: 3\n",
      "for test: 1\n",
      "\n",
      "Preparing dataset for Training\n",
      "\n",
      "Adding austin4 to AOI list\n",
      "\n",
      "Adding tyrol-w25 to AOI list\n",
      "\n",
      "Adding chicago5 to AOI list\n",
      "\n",
      "Adding vienna10 to AOI list\n",
      "\n",
      "Adding chicago28 to AOI list\n",
      "\n",
      "Adding vienna16 to AOI list\n",
      "\n",
      "Adding chicago33 to AOI list\n",
      "\n",
      "Adding kitsap18 to AOI list\n",
      "\n",
      "Adding kitsap12 to AOI list\n",
      "\n",
      "Adding chicago11 to AOI list\n",
      "\n",
      "Adding kitsap17 to AOI list\n",
      "\n",
      "Adding austin14 to AOI list\n",
      "\n",
      "Adding tyrol-w21 to AOI list\n",
      "\n",
      "Adding kitsap1 to AOI list\n",
      "\n",
      "Preparing dataset for Validation\n",
      "\n",
      "Adding austin29 to AOI list\n",
      "\n",
      "Adding kitsap31 to AOI list\n",
      "\n",
      "Adding tyrol-w5 to AOI list\n"
     ]
    }
   ],
   "source": [
    "dataset_path = config.dataset_path    \n",
    "exp_name = config.exp_name\n",
    "\n",
    "train, val, test = data_loader.get_samples(dataset_path)\n",
    "\n",
    "print(\"\\nPreparing dataset for Training\")\n",
    "X_train, y_train = data_loader.build_source(train, dataset_path)\n",
    "\n",
    "print(\"\\nPreparing dataset for Validation\")\n",
    "X_val, y_val = data_loader.build_source(val, dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the config.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Params\n",
    "tile_size = config.tile_size\n",
    "no_of_samples = config.no_of_samples\n",
    "downs = config.down_sampling\n",
    "\n",
    "batch_size = config.batch_size\n",
    "epochs = config.epochs  \n",
    "initial_epoch = config.initial_epoch\n",
    "\n",
    "training_frm_scratch = config.training_frm_scratch \n",
    "training_frm_chkpt = config.training_frm_chkpt \n",
    "transfer_lr = config.transfer_lr\n",
    "\n",
    "if sum((training_frm_scratch, training_frm_chkpt, transfer_lr)) != 1:\n",
    "    raise Exception(\"Conflicting training modes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a super set for loss, optimiser and metric functions. The user can select any from there options using the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_class = {'bin_cross': 'binary_crossentropy',\n",
    "              'bce_dice': bce_dice_loss}\n",
    "\n",
    "metric_class = {'dice':dice_coeff}\n",
    "\n",
    "optimiser_class = {'adam': (Adam, {}), \n",
    "                   'nadam': (Nadam, {}), \n",
    "                   'rmsprop': (RMSprop, {}),\n",
    "                   'sgd':(SGD, {'decay':1e-6, 'momentum':0.99, 'nesterov':True})} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the no of iterations we will use per epoch.  For training, the steps per epoch is multiplied by 2 as we are using augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spe = int(np.floor((len(X_train)*no_of_samples*2) / batch_size)) #spe = Steps per epoch\n",
    "val_spe = int(np.floor((len(X_val)*no_of_samples*2) / batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising the datagenerators for training and validation. \n",
    "\n",
    "Get to know more about keras generator from here: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise generators    \n",
    "train_generator = SegDataGenerator(dataset_path, img_source=X_train, \n",
    "                                mask_source=y_train, batch_size= batch_size, \n",
    "                                no_of_samples = no_of_samples, tile_size= tile_size, \n",
    "                                downsampling_factor = downs)\n",
    "\n",
    "val_generator = SegDataGenerator(dataset_path, img_source=X_val, \n",
    "                                mask_source=y_val, batch_size= batch_size, \n",
    "                                no_of_samples = no_of_samples, tile_size= tile_size, \n",
    "                                downsampling_factor = downs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training from scratch\n"
     ]
    }
   ],
   "source": [
    "if training_frm_scratch:\n",
    "    print(\"Training from scratch\")\n",
    "    optimizer = optimiser_class[config.optimiser][0](lr=config.learning_rate, \n",
    "                               **optimiser_class[config.optimiser][1])\n",
    "    loss = loss_class[config.loss]\n",
    "    metric = metric_class[config.metric]\n",
    "    \n",
    "    if config.no_of_gpu > 1:\n",
    "        print(\"Running in multi-gpu mode\")\n",
    "        with tf.device('/cpu:0'):\n",
    "            build = getattr(importlib.import_module(config.model),\"build\")\n",
    "            model = build(size = config.tile_size, chs = 3)\n",
    "        \n",
    "        gpu_model = multi_gpu_model(model, gpus = config.no_of_gpu)\n",
    "        gpu_model.compile(loss= loss, optimizer=optimizer, metrics=[metric, 'accuracy'])\n",
    "        model.compile(loss= loss, optimizer=optimizer, metrics=[metric, 'accuracy'])\n",
    "        \n",
    "    else:\n",
    "        build = getattr(importlib.import_module(config.model),\"build\")\n",
    "        model = build(size = config.tile_size, chs = 3)\n",
    "        model.compile(loss= loss, optimizer=optimizer, metrics=[metric, 'accuracy'])\n",
    "        gpu_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume training\n",
    "\n",
    "When we load a keras model using `load_model`, we do not need to compile it as it _returns_ a **compiled model**. <br>\n",
    "**\"load_model\"** in keras does 4 things: \n",
    " - loads architecure, \n",
    " - loads weights, \n",
    " - loads optimisers and loss, \n",
    " - loads state of optimiser and loss\n",
    " \n",
    "\n",
    "To know more about keras models api: https://keras.io/models/about-keras-models/#about-keras-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_frm_chkpt:\n",
    "    print(\"Training from prv checkpoint\")\n",
    "    model_path = config.model_path\n",
    "    model = load_model(model_path, \n",
    "                       custom_objects={'bce_dice_loss': bce_dice_loss, 'dice_coeff':dice_coeff}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning: \n",
    "for Transfer Learning we follow the sequence: <br> \n",
    "<font size=\"5\">build --> load_weights --> finetune --> compile </font>\n",
    "\n",
    "Note: Compiling a model only defines the loss function, the optimizer and the metrics. That's all. \n",
    "Weights after compilation are the same as before compilation.\n",
    "\n",
    "#### To check the weights:\n",
    ">for layer in model.layers: <br>\n",
    ">>    weights = layer.get_weights() <br>\n",
    ">>    print(weights) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if transfer_lr:\n",
    "    print(\"Transfer Learning mode\")\n",
    "    \n",
    "    #build the model\n",
    "    model_path = config.model_path\n",
    "    gpu_model = load_model(model_path, \n",
    "                           custom_objects={'bce_dice_loss': bce_dice_loss,\n",
    "                                           'dice_coeff':dice_coeff}) \n",
    "     \n",
    "    build = getattr(importlib.import_module(config.model),\"build\")\n",
    "    model = build(size = config.tile_size, chs = 3)\n",
    "    model.set_weights(gpu_model.layers[-2].get_weights())\n",
    "        \n",
    "    #freeze layers for transfer learning & load weights\n",
    "    model = finetune_model(model)\n",
    "        \n",
    "    if config.no_of_gpu > 1:\n",
    "        gpu_model = multi_gpu_model(model, gpus = config.no_of_gpu, cpu_relocation=True)\n",
    "        print(\"Running in multi-gpu mode\")\n",
    "    else:\n",
    "        gpu_model = model\n",
    "                     \n",
    "    #compile the model\n",
    "    gpu_model = compile_model(gpu_model, lr = config.learning_rate,\n",
    "                              optimiser = optimiser_class[config.optimiser],\n",
    "                              loss = loss_class[config.loss] , \n",
    "                              metric = metric_class[config.metric]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the callbacks to be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set callbacks        \n",
    "callbacks_list = get_callbacks(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start/Resume training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = gpu_model.fit_generator(steps_per_epoch= train_spe,\n",
    "                generator=train_generator,\n",
    "                epochs=epochs,\n",
    "                validation_data = val_generator,\n",
    "                validation_steps = val_spe,\n",
    "                initial_epoch = initial_epoch,\n",
    "                callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save final complete model        \n",
    "filename = \"model_ep_\"+str(int(epochs))+\"_batch_\"+str(int(batch_size))\n",
    "model.save(\"./data/\"+exp_name+\"/\"+filename+\".h5\")\n",
    "print(\"Saved complete model file at: \", filename+\"_model\"+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save history\n",
    "history_to_save = generate_stats(history, config)\n",
    "pd.DataFrame(history_to_save).to_csv(\"./data/\"+exp_name+\"/\"+filename + \"_train_results.csv\")\n",
    "save_plots(history, exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
